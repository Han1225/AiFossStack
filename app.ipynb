{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target implement RAG\n",
    "\n",
    "# pending features:\n",
    "# restrict to high score document threshhold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d8d4ac1a-c40e-41d6-9574-16a89226d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1fe46651-707d-46d0-bcbe-b99cee210b6d",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url=\"http://localhost:11434\"\n",
    "model=\"zephyr\"  #orca-mini , mistral, or zephyr\n",
    "\n",
    "llm = Ollama(base_url=base_url, model=model, \n",
    "callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed7367-2905-4b5c-af6c-c28575bd076f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#llm(\"Hola!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory loader, including text, pdf\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "input_dir = \"./data/\"\n",
    "data = DirectoryLoader(input_dir , use_multithreading=True).load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split it into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize & store\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "index_name = \"index\"\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings(), persist_directory=\"./indexes/\" + index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load chromadb vectorstore from disk <-works\n",
    "index_name = \"index\"\n",
    "db2 = Chroma(persist_directory=\"./indexes/\" + index_name , embedding_function=GPT4AllEmbeddings())\n",
    "print(\"Rows:\", db2._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk & query <-works\n",
    "index_name = \"index\"\n",
    "db3 = Chroma(persist_directory=\"./indexes/\" + index_name, embedding_function=GPT4AllEmbeddings())\n",
    "query = \"hello\"\n",
    "docs = db3.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prompt does not require the template library\n",
    "system_prompt = '''\n",
    "You are an expert. Write your answer following these criteria:\n",
    "* Respond exclusively based on the documents provided in the {context}.\n",
    "* Cite the exact source next to each paragraph.\n",
    "* Indicate date, location, and entities related to each fact you cite.\n",
    "* Write in an elegant, professional, diplomatic style.\n",
    "\n",
    "If the context documents provided do not contain the answer:\n",
    "* do not generate a response based on your neural network, \n",
    "* instead respond with this sentence exactly: \"The knowledge base does not have enough information about your question.\" \n",
    "\n",
    "This is the question you are responding: \\n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM RAG query <-works with prompt above. Without using a template | successfully says it does not have enough info, however it adds a response based on the neural network.\n",
    "from langchain.chains import RetrievalQA\n",
    "# expose this index in a retriever interface\n",
    "retriever = db2.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})   #k = 10 gives nice results # can use \"mmr\" or \"similarity\"\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "question = '''\n",
    "Why is the sky blue?'''\n",
    "\n",
    "result = qa({\"query\": system_prompt + question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result # <-- works\n",
    "# result[\"source_documents\"] # <-works\n",
    "#result[\"source_documents\"][:] # <-- works\n",
    "#result[\"source_documents\"][0].metadata # <-- works\n",
    "# for source in result[\"source_documents\"][:]: # <-- works\n",
    "#     print(source.metadata)\n",
    "\n",
    "#print sources\n",
    "for source in result[\"source_documents\"][:]: # <-- works\n",
    "    print(source.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are an expert. Execute the instruction given to you following these criteria:\\n* Cite the exact source next to each paragraph.\\n* Indicate date, location, and entities related to each fact you cite.\\n* Write in an elegant, professional, diplomatic style. \\nUse the following pieces of context: {context} to respond the instruction in this: {question}.\\nIf the context provided does not contain the answer:\\n* do not generate a response, \\n* instead respond with this sentence exactly: \"The knowledge base does not have enough information about your question.\\n'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM RAG with system prompt <-- works. Lesson-learned: the content in the template takes priority over the system prompt.\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Build prompt\n",
    "system_prompt = '''\n",
    "You are an expert. Execute the instruction given to you following these criteria:\n",
    "* Cite the exact source next to each paragraph.\n",
    "* Indicate date, location, and entities related to each fact you cite.\n",
    "* Write in an elegant, professional, diplomatic style. \n",
    "'''\n",
    "template = system_prompt + '''Use the following pieces of context: {context} to respond the instruction in this: {question}.\n",
    "If the context provided does not contain the answer:\n",
    "* do not generate a response, \n",
    "* instead respond with this sentence exactly: \"The knowledge base does not have enough information about your question.\n",
    "''' \n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)# Run chain\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "# expose this index in a retriever interface\n",
    "retriever = db2.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})   #k = 10 gives nice results # can use \"mmr\" or \"similarity\"\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, retriever=retriever, return_source_documents=True, chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with above cell. It is very strict if it does not find the exact answer it says so.\n",
    "question = \"How does a person learns how to dance\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "# Check the result of the query\n",
    "result[\"result\"]\n",
    "# Check the source document from where we \n",
    "result[\"source_documents\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R & D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search with score <--- works\n",
    "query = \"what is the meaning of ...?\"\n",
    "docs = db2.similarity_search_with_score(query) #lower the score the more similar. \n",
    "docs\n",
    "#print(docs[0].page_content)\n",
    "#print(docs[0].metadata)\n",
    "#docs[0]\n",
    "#len(docs) #by default, the top 4 results are returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete collection\n",
    "#print(\"Count:\", db2._collection.count())\n",
    "#db2.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
